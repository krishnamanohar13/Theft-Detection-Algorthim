{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the necessary libraries"
      ],
      "metadata": {
        "id": "KM1odNRIvfLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-io\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.applications import VGG16"
      ],
      "metadata": {
        "id": "_e-3FYUAtsrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieving the list of all class names from the custom dataset uploaded on kaggle"
      ],
      "metadata": {
        "id": "Y-KSqWmstx8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = os.listdir('/kaggle/input/suspicious-activity-vandalism/Suspicious_activity 2/Dataset')\n",
        "classes"
      ],
      "metadata": {
        "id": "sFfG7xRXt3Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomly Selecting and Displaying Video Frames from Dataset"
      ],
      "metadata": {
        "id": "X3fA1vp5vaxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Selecting randomness\n",
        "seed_constant = 3\n",
        "np.random.seed(seed_constant)\n",
        "random.seed(seed_constant)\n",
        "tf.random.set_seed(seed_constant)\n",
        "\n",
        "plt.figure(figsize = (20, 20))\n",
        "\n",
        "# Displaying Random Video Frames from Dataset\n",
        "for counter, index in enumerate(range(len(classes)), 1):\n",
        "    selected_class = classes[index]\n",
        "    video_files_list = os.listdir(f'/kaggle/input/suspicious-activity-vandalism/Suspicious_activity 2/Dataset/{selected_class}')\n",
        "    selected_video_file = random.choice(video_files_list)\n",
        "\n",
        "    video_reader = cv2.VideoCapture(f'/kaggle/input/suspicious-activity-vandalism/Suspicious_activity 2/Dataset/{selected_class}/{selected_video_file}')\n",
        "    video_reader.set(1, 25)\n",
        "\n",
        "    _, bgr_frame = video_reader.read()\n",
        "    bgr_frame = cv2.resize(bgr_frame ,(224,224))\n",
        "\n",
        "    video_reader.release()\n",
        "\n",
        "    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Writing the class name on the video frame.\n",
        "    cv2.putText(rgb_frame, selected_class, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "    # Displaying the frame.\n",
        "    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')"
      ],
      "metadata": {
        "id": "hut5oRRqt9G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specifying Data Variables"
      ],
      "metadata": {
        "id": "XV3m6wnTvWfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/suspicious-activity-vandalism/Suspicious_activity 2/Dataset\"\n",
        "\n",
        "img_height = 64\n",
        "img_width = 64\n",
        "\n",
        "seq_len = 30"
      ],
      "metadata": {
        "id": "91KpVzVsuExk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing and creating dataset"
      ],
      "metadata": {
        "id": "lE5mxzjqvSbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extracting_frames(path):\n",
        "\n",
        "    frames_list = []\n",
        "    video_reader = cv2.VideoCapture(path)\n",
        "    frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    skipped_frames = max(int(frame_count/seq_len), 1)\n",
        "\n",
        "\n",
        "    # Creating an ImageDataGenerator object to perform data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "         # Adjust brightness\n",
        "        brightness_range=(0.8, 1.2),\n",
        "        # Apply shear transformation\n",
        "        shear_range=10,\n",
        "        # Apply random channel shifts\n",
        "        channel_shift_range=20,\n",
        "        # Handling boundary pixels during transformations\n",
        "        fill_mode='reflect'\n",
        "    )\n",
        "\n",
        "\n",
        "    for frame_counter in range(seq_len):\n",
        "\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skipped_frames)\n",
        "        success, frame = video_reader.read()\n",
        "\n",
        "\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        resized_frame = cv2.resize(frame, (img_height, img_width))\n",
        "        aug_frame = datagen.random_transform(resized_frame)\n",
        "        normalized_frame = aug_frame / 255\n",
        "        frames_list.append(normalized_frame)\n",
        "\n",
        "    video_reader.release()\n",
        "\n",
        "    return frames_list\n",
        "def dataset_creation():\n",
        "\n",
        "    features = []\n",
        "    paths = []\n",
        "    labels = []\n",
        "\n",
        "    for index_of_class, name_of_class in enumerate(classes):\n",
        "\n",
        "        print('Extracting Data of Class:', name_of_class)\n",
        "        list_of_files = os.listdir(os.path.join(dataset_path, name_of_class))\n",
        "\n",
        "        for file in list_of_files:\n",
        "\n",
        "            video_file_path = os.path.join(dataset_path, name_of_class, file)\n",
        "            frames = extracting_frames(video_file_path)\n",
        "\n",
        "            if len(frames) == seq_len:\n",
        "                features.append(frames)\n",
        "                labels.append(index_of_class)\n",
        "                paths.append(video_file_path)\n",
        "\n",
        "    features = np.asarray(features)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return features, paths, labels\n",
        "features, paths, labels = dataset_creation()\n",
        "print(features.shape, labels.shape)"
      ],
      "metadata": {
        "id": "EO6Db4xnuKX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One Hot Encoding and Data Splitting"
      ],
      "metadata": {
        "id": "e0T6HSieuOL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoded_labels = to_categorical(labels)\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.25, shuffle = True, random_state = seed_constant)"
      ],
      "metadata": {
        "id": "lwOsEHcHuSTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation\n"
      ],
      "metadata": {
        "id": "SH5zszytukKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def creating_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    vgg = VGG16(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
        "    for layer in vgg.layers:\n",
        "        layer.trainable = False\n",
        "    model.add(TimeDistributed(vgg, input_shape=(seq_len, img_height, img_width, 3)))\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu'), input_shape = (seq_len, img_height, img_width, 3)))\n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "    model.add(LSTM(32))\n",
        "\n",
        "    model.add(Dense(len(classes), activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "# %% [code] {\"scrolled\":true,\"jupyter\":{\"outputs_hidden\":false}}\n",
        "num_classes = len(classes)\n",
        "lstm_models = []\n",
        "num_models = 6\n",
        "\n",
        "# Training multiple LSTM models with initialization or hyperparameters\n",
        "training_histories = []\n",
        "for i in range(num_models):\n",
        "    lstm_model = creating_model()\n",
        "    lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    early_stopping_callback = EarlyStopping(monitor = 'accuracy', patience = 10, mode = 'max', restore_best_weights = True)\n",
        "    training_history = lstm_model.fit(x = features_train, y = labels_train, epochs = 30, batch_size = 4 , shuffle = True, validation_split = 0.25, callbacks = [early_stopping_callback])\n",
        "    lstm_models.append(lstm_model)\n",
        "    training_histories.append(training_history)\n",
        "\n",
        "# Combining the predictions of the multiple LSTM models using majority voting\n",
        "y_pred = []\n",
        "for i in range(len(features_test)):\n",
        "    predictions = []\n",
        "    for model in lstm_models:\n",
        "        predictions.append(np.argmax(model.predict(np.expand_dims(features_test[i],axis =0))[0]))\n",
        "    y_pred.append(max(set(predictions), key=predictions.count))\n",
        "\n",
        "\n",
        "# Converting the predicted labels to one-hot encoding\n",
        "y_pred = tf.keras.utils.to_categorical(y_pred, num_classes)"
      ],
      "metadata": {
        "id": "NRTCVToqus-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy and Model Saving"
      ],
      "metadata": {
        "id": "3V20Z5hPu7X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "accuracy.update_state(labels_test, y_pred)\n",
        "print(f\"Ensemble model accuracy: {accuracy.result().numpy()}\")\n",
        "model.save(\"/kaggle/working/Suspicious_Human_Activity_Detection.h5\")"
      ],
      "metadata": {
        "id": "B9lECStSu_Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video Annonation"
      ],
      "metadata": {
        "id": "LoxYng4vvByY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Video_Annonation(input_path, output_path, seq_len):\n",
        "\n",
        "    vid_reader = cv2.VideoCapture(input_path)\n",
        "\n",
        "    Original_width = int(vid_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    Original_height = int(vid_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    vid_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'),\n",
        "                                   vid_reader.get(cv2.CAP_PROP_FPS), (Original_width, Original_height))\n",
        "\n",
        "    queue = deque(maxlen = seq_len)\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "         # Adjust brightness\n",
        "        brightness_range=(0.8, 1.2),\n",
        "        # Apply shear transformation\n",
        "        shear_range=10,\n",
        "        # Apply random channel shifts\n",
        "        channel_shift_range=20,\n",
        "        # Handling boundary pixels during transformations\n",
        "        fill_mode='reflect'\n",
        "    )\n",
        "\n",
        "    prediction = ''\n",
        "\n",
        "    while vid_reader.isOpened():\n",
        "\n",
        "        ok, frame = vid_reader.read()\n",
        "\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "        resized_frame = cv2.resize(frame, (img_height, img_width))\n",
        "        aug_frame = datagen.random_transform(resized_frame)\n",
        "        normalized_frame = aug_frame / 255\n",
        "\n",
        "        queue.append(normalized_frame)\n",
        "\n",
        "        if len(queue) == seq_len:\n",
        "\n",
        "            predicted_label = np.argmax(model.predict(np.expand_dims(queue, axis = 0))[0])\n",
        "\n",
        "            prediction = classes[predicted_label]\n",
        "\n",
        "        cv2.putText(frame, prediction, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        vid_writer.write(frame)\n",
        "\n",
        "    vid_reader.release()\n",
        "    vid_writer.release()\n",
        "input_path = \"/kaggle/input/suspicious-activity-vandalism/Suspicious_activity 2/Dataset/Vandalism/15r.mp4\"\n",
        "output_path = \"/kaggle/working/15r.mp4\"\n",
        "Video_Annonation(input_path, output_path, seq_len)"
      ],
      "metadata": {
        "id": "eDGIF_ZWvF4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motionless Bag Detection"
      ],
      "metadata": {
        "id": "4eNS41S4vJaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ultralytics\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Load YOLOv5 model\n",
        "device = torch.device('cpu')\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(device)\n",
        "\n",
        "# Define the class labels to detect\n",
        "class_labels = [24, 28, 26]\n",
        "\n",
        "# Load video\n",
        "video_path = '/kaggle/working/15r.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Define output video writer\n",
        "output_path = '/kaggle/working/Final_video.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "# Get class names associated with YOLOv5 model\n",
        "class_names = model.module.names if hasattr(model, 'module') else model.names\n",
        "\n",
        "# Variables for bag movement detection\n",
        "threshold = 1000  # Threshold value for bag movement\n",
        "duration_threshold = 2  # Duration threshold in seconds\n",
        "bag_position = None  # Position of the bag (center of bounding box)\n",
        "duration = 0  # Duration of the bag within the threshold\n",
        "last_detection_time = 0  # Time of the last detection\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform inference on the frame\n",
        "    results = model(frame)\n",
        "\n",
        "    # Get bounding box coordinates and labels\n",
        "    boxes = results.pred[0].detach().cpu().numpy()[:, :4]\n",
        "    labels = results.pred[0].detach().cpu().numpy()[:, -1]\n",
        "\n",
        "    # Filter the bounding boxes and labels based on class labels\n",
        "    filtered_boxes = []\n",
        "    filtered_labels = []\n",
        "    for box, label in zip(boxes, labels):\n",
        "        if label in class_labels:\n",
        "            filtered_boxes.append(box)\n",
        "            filtered_labels.append(class_names[int(label)])  # Get class name based on label\n",
        "\n",
        "    # Draw bounding boxes and labels on the frame\n",
        "    for box, label in zip(filtered_boxes, filtered_labels):\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        # cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Check if bag is within threshold\n",
        "    if len(filtered_boxes) > 0:\n",
        "        x1, y1, x2, y2 = filtered_boxes[0].astype(int)\n",
        "        bag_x = (x1 + x2) // 2  # Calculate bag's x-coordinate\n",
        "        bag_y = (y1 + y2) // 2  # Calculate bag's y-coordinate\n",
        "\n",
        "        if bag_position is None:\n",
        "            bag_position = (bag_x, bag_y)\n",
        "            last_detection_time = time.time()\n",
        "        else:\n",
        "            # Calculate Euclidean distance between current and previous position\n",
        "            distance = np.sqrt((bag_x - bag_position[0]) ** 2 + (bag_y - bag_position[1]) ** 2)\n",
        "\n",
        "            if distance <= threshold:\n",
        "                duration += 1 / fps  # Increase duration by frame duration\n",
        "\n",
        "                if duration >= duration_threshold:\n",
        "                    # Bounding box color becomes red\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                    cv2.putText(frame, str(int(duration)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
        "                                (0, 0, 255), 2)\n",
        "            else:\n",
        "                current_time = time.time()\n",
        "                elapsed_time = current_time - last_detection_time\n",
        "\n",
        "                if elapsed_time >= 1:\n",
        "                    # Bounding box color becomes green, restart duration\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                    cv2.putText(frame, str(int(duration)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
        "                                (0, 255, 0), 2)\n",
        "                    bag_position = (bag_x, bag_y)\n",
        "                    duration = 0\n",
        "                    last_detection_time = current_time\n",
        "\n",
        "    # Write annotated frame to output video\n",
        "    out.write(frame)\n",
        "\n",
        "# Release video capture and writer\n",
        "cap.release()\n",
        "out.release()"
      ],
      "metadata": {
        "id": "dwCUkEDVvNEL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}